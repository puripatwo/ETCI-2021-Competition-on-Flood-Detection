{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses [this blog post](https://medium.com/cloud-to-street/jumpstart-your-machine-learning-satellite-competition-submission-2443b40d0a5a) and [this video](https://www.youtube.com/watch?v=SsnWM1xWDu4) as references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mz4KCyB4Pw6Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/etci_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import ttach as tta\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1QXI66oQ0z_"
   },
   "source": [
    "## Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOi7wYOKQyjF",
    "outputId": "916e5607-3933-41d5-fefb-70b0a1e7814b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test temporal-regions: 14\n"
     ]
    }
   ],
   "source": [
    "# path to dataset root directory\n",
    "dset_root = 'ETCI-2021-Flood-Detection/data/'\n",
    "test_dir = os.path.join(dset_root, 'test_internal')\n",
    "n_test_regions = len(glob(test_dir+'/*/'))\n",
    "print('Number of test temporal-regions: {}'.format(n_test_regions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWNe2TxWVrQ1"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lHnXysQzVtr_"
   },
   "outputs": [],
   "source": [
    "def get_test_id(path):\n",
    "    return path.split(\"_\")[0] + \"_\" + path.split(\"_\")[1]\n",
    "\n",
    "def make_im_name(id, suffix):\n",
    "    return id.split(\".\")[0] + f\"_{suffix}.png\"\n",
    "\n",
    "def s1_to_rgb(vv_image, vh_image):\n",
    "    ratio_image = np.clip(np.nan_to_num(vh_image/vv_image, 0), 0, 1)\n",
    "    rgb_image = np.stack((vv_image, vh_image, 1-ratio_image), axis=2)\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMhA2NnmQ5-G"
   },
   "source": [
    "## Create dataframe\n",
    "\n",
    "As per [the competition website](https://nasa-impact.github.io/etci2021/), the submission file needs to be generated following a particular sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /opt/anaconda3/envs/etci_env/lib/python3.11/site-packages (3.2)\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://git.io/JsRTE -O test_sentinel.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# url = \"https://git.io/JsRTE\"\n",
    "# r = requests.get(url)\n",
    "# with open(\"test_sentinel.csv\", \"wb\") as f:\n",
    "#     f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B2oExzdPiPfR",
    "outputId": "d955d599-b1be-485c-88c0-214444e10086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12348, 2)\n"
     ]
    }
   ],
   "source": [
    "test_file_sequence = pd.read_csv(\"test_sentinel.csv\", header=None)\n",
    "test_file_sequence = test_file_sequence.values.squeeze().tolist()\n",
    "\n",
    "all_test_vv = [os.path.join(test_dir, get_test_id(id), \"tiles\", \"vv\", make_im_name(id, \"vv\")) \n",
    "                                                                for id in test_file_sequence]\n",
    "all_test_vh = [os.path.join(test_dir, get_test_id(id), \"tiles\", \"vh\", make_im_name(id, \"vh\")) \n",
    "                                                                for id in test_file_sequence]\n",
    "\n",
    "paths = {'vv_image_path': all_test_vv,\n",
    "         'vh_image_path': all_test_vh,\n",
    "}\n",
    "\n",
    "test_df = pd.DataFrame(paths)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "vv_image_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "vh_image_path",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6c29d8fe-9dbd-434d-8333-e38a6c95f41e",
       "rows": [
        [
         "0",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vv/redrivernorth_20190104t002247_x-0_y-0_vv.png",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vh/redrivernorth_20190104t002247_x-0_y-0_vh.png"
        ],
        [
         "1",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vv/redrivernorth_20190104t002247_x-0_y-1_vv.png",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vh/redrivernorth_20190104t002247_x-0_y-1_vh.png"
        ],
        [
         "2",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vv/redrivernorth_20190104t002247_x-0_y-10_vv.png",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vh/redrivernorth_20190104t002247_x-0_y-10_vh.png"
        ],
        [
         "3",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vv/redrivernorth_20190104t002247_x-0_y-11_vv.png",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vh/redrivernorth_20190104t002247_x-0_y-11_vh.png"
        ],
        [
         "4",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vv/redrivernorth_20190104t002247_x-0_y-12_vv.png",
         "../ETCI-2021-Flood-Detection/data/test_internal/redrivernorth_20190104t002247/tiles/vh/redrivernorth_20190104t002247_x-0_y-12_vh.png"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vv_image_path</th>\n",
       "      <th>vh_image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "      <td>../ETCI-2021-Flood-Detection/data/test_interna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       vv_image_path  \\\n",
       "0  ../ETCI-2021-Flood-Detection/data/test_interna...   \n",
       "1  ../ETCI-2021-Flood-Detection/data/test_interna...   \n",
       "2  ../ETCI-2021-Flood-Detection/data/test_interna...   \n",
       "3  ../ETCI-2021-Flood-Detection/data/test_interna...   \n",
       "4  ../ETCI-2021-Flood-Detection/data/test_interna...   \n",
       "\n",
       "                                       vh_image_path  \n",
       "0  ../ETCI-2021-Flood-Detection/data/test_interna...  \n",
       "1  ../ETCI-2021-Flood-Detection/data/test_interna...  \n",
       "2  ../ETCI-2021-Flood-Detection/data/test_interna...  \n",
       "3  ../ETCI-2021-Flood-Detection/data/test_interna...  \n",
       "4  ../ETCI-2021-Flood-Detection/data/test_interna...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW5yZ_I0RFK6"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Gus4cgUzdx3o"
   },
   "outputs": [],
   "source": [
    "class ETCIDataset(Dataset):\n",
    "    def __init__(self, dataframe, split, transform=None):\n",
    "        self.split = split\n",
    "        self.dataset = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        \n",
    "        df_row = self.dataset.iloc[index]\n",
    "\n",
    "        # load vv and vh images\n",
    "        vv_image = cv2.imread(df_row['vv_image_path'], 0) / 255.0\n",
    "        vh_image = cv2.imread(df_row['vh_image_path'], 0) / 255.0\n",
    "        \n",
    "        # convert vv and ch images to rgb\n",
    "        rgb_image = s1_to_rgb(vv_image, vh_image)\n",
    "\n",
    "        if self.split == 'test':\n",
    "            # no flood mask should be available\n",
    "            example['image'] = rgb_image.transpose((2,0,1)).astype('float32')\n",
    "            example['vv_image_path'] = df_row['vv_image_path']\n",
    "            example['vh_image_path'] = df_row['vh_image_path']\n",
    "        else:\n",
    "            # load ground truth flood mask\n",
    "            flood_mask = cv2.imread(df_row['flood_label_path'], 0) / 255.0\n",
    "\n",
    "            # compute transformations\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=rgb_image, mask=flood_mask)\n",
    "                rgb_image = augmented['image']\n",
    "                flood_mask = augmented['mask']\n",
    "\n",
    "            example['image'] = rgb_image.transpose((2,0,1)).astype('float32')\n",
    "            example['mask'] = flood_mask.astype('int64')\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls9VrVu1d3Ba"
   },
   "outputs": [],
   "source": [
    "test_dataset = ETCIDataset(test_df, split='test', transform=None)\n",
    "\n",
    "batch_size = 96 * torch.cuda.device_count()\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                         num_workers=os.cpu_count(), pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Uc2z3thWRG5Q"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling and pseudo labeling\n",
    "\n",
    "We start by defining the model classes and the paths to their pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_mobilenet = smp.Unet(\n",
    "    encoder_name=\"mobilenet_v2\", \n",
    "    encoder_weights=None, \n",
    "    in_channels=3,                  \n",
    "    classes=2                      \n",
    ")\n",
    "\n",
    "upp_mobilenet = smp.UnetPlusPlus(\n",
    "    encoder_name=\"mobilenet_v2\", \n",
    "    encoder_weights=None, \n",
    "    in_channels=3,                  \n",
    "    classes=2                      \n",
    ")\n",
    "\n",
    "model_defs = [unet_mobilenet, upp_mobilenet]\n",
    "model_paths = [\"src/model/round_0/unet_mobilenet_v2_0.pth\",\n",
    "              \"src/model/round_0/upp_mobilenet_v2_0.pth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: After a round of pseudo-labeling, one should also incorporate the latest fine-tuned model (obtained by running the `src/train_pseudo_label.py` script) in the ensemble for better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_single(model_def, weights, dir_path, conf_thres=0.95, pixel_thres=0.9):\n",
    "    models = []\n",
    "    for model_def, weight in zip(model_defs, weights):\n",
    "        model_def.load_state_dict(torch.load(weight))\n",
    "        model = tta.SegmentationTTAWrapper(model_def, tta.aliases.d4_transform(), merge_mode=\"mean\") \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        models.append(model)\n",
    "    \n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    vv_s = []\n",
    "    vh_s = []\n",
    "    masks = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            # load image and mask into device memory\n",
    "            image = batch['image'].to(device)\n",
    "\n",
    "            # pass images into model\n",
    "            preds = []\n",
    "            for model in models:\n",
    "                pred = model(image)\n",
    "                preds.append(pred.detach().cpu().numpy())\n",
    "            \n",
    "            preds = np.array(preds)\n",
    "            preds = np.mean(preds, axis=0) # Mean over ensembles\n",
    "            \n",
    "            filter_preds, _ = nn.Softmax(dim=1)(torch.tensor(preds)).max(1) # Apply softmax -> take max\n",
    "            # Shape: (batch_size, 256, 256)\n",
    "            filter_preds = filter_preds.numpy()\n",
    "            \n",
    "            # Are `pixel_thres`% of the pixels in an entry greater `conf_thres`? \n",
    "            filtered = np.sum(filter_preds > conf_thres, axis = (1, 2)) > pixel_thres * 256 * 256 # 256: image size\n",
    "\n",
    "            for idx, filter_ in enumerate(filtered): # entries: (batch_size, 256, 256)\n",
    "                if filter_:\n",
    "                    vv_s.append(batch['vv_image_path'][idx])\n",
    "                    vh_s.append(batch['vh_image_path'][idx])\n",
    "                    entry = nn.Softmax(dim=0)(torch.tensor(preds[idx])).argmax(0).numpy() * 255. \n",
    "                    \n",
    "                    pseudo_path = \"_\".join(batch['vv_image_path'][idx].split(\"/\")[-1].split(\"_\")[:-1]) + \".png\"\n",
    "                    pseudo_path = os.path.join(dir_path, pseudo_path)\n",
    "                    masks.append(pseudo_path)\n",
    "                    cv2.imwrite(pseudo_path, entry.astype(\"float32\"))\n",
    "                    \n",
    "    return vv_s, vh_s, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_s, vh_s, masks =  get_predictions_single(model_defs, model_paths, \"pseudo_labels\")\n",
    "assert len(vv_s) == len(vh_s) == len(masks)\n",
    "\n",
    "paths = {'vv_image_path': vv_s,\n",
    "         'vh_image_path': vh_s,\n",
    "         'flood_label_path': masks\n",
    "}\n",
    "\n",
    "pseudo_df = pd.DataFrame(paths)\n",
    "print(pseudo_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_df.to_csv(\"pseudo_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe is then used to retrain any of the model used in the ensemble here (refer to `src/train_pseudo_label.py`). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ECTI_Jumpstart.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "etci_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
